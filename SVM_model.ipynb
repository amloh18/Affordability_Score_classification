{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/amarjotsinghlohia/Documents/Dissertation/Data/home-credit-default-risk/application_train_upsampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of data into Target and non_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336226, 57) (336226,)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,3:]\n",
    "y = data['TARGET']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Here we are going to use scaling of the dataset for better prediction. Scaling normalizes the data to prroduce mean as 1 and standard deviation as 0. This helps the model to execute faster and more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scale(X_train)\n",
    "X_test_scaled = scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 22.65, NNZs: 57, Bias: 0.051195, T: 252169, Avg. loss: 3.443944\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.55, NNZs: 57, Bias: -0.048817, T: 504338, Avg. loss: 1.159291\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.94, NNZs: 57, Bias: 0.081720, T: 756507, Avg. loss: 1.023711\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.09, NNZs: 57, Bias: -0.169523, T: 1008676, Avg. loss: 0.971360\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.99, NNZs: 57, Bias: -0.143146, T: 1260845, Avg. loss: 0.939654\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.46, NNZs: 57, Bias: -0.078672, T: 1513014, Avg. loss: 0.922489\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.98, NNZs: 57, Bias: 0.087221, T: 1765183, Avg. loss: 0.909336\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.54, NNZs: 57, Bias: 0.014222, T: 2017352, Avg. loss: 0.900962\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.31, NNZs: 57, Bias: 0.032447, T: 2269521, Avg. loss: 0.893677\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.98, NNZs: 57, Bias: 0.138996, T: 2521690, Avg. loss: 0.887074\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.95, NNZs: 57, Bias: 0.071681, T: 2773859, Avg. loss: 0.883150\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.85, NNZs: 57, Bias: 0.118114, T: 3026028, Avg. loss: 0.878920\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.81, NNZs: 57, Bias: 0.099637, T: 3278197, Avg. loss: 0.875605\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.71, NNZs: 57, Bias: -0.004001, T: 3530366, Avg. loss: 0.872404\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.64, NNZs: 57, Bias: 0.025378, T: 3782535, Avg. loss: 0.870956\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.54, NNZs: 57, Bias: 0.063583, T: 4034704, Avg. loss: 0.869427\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.51, NNZs: 57, Bias: 0.076255, T: 4286873, Avg. loss: 0.868264\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.53, NNZs: 57, Bias: 0.039136, T: 4539042, Avg. loss: 0.865095\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.44, NNZs: 57, Bias: 0.026945, T: 4791211, Avg. loss: 0.863958\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.42, NNZs: 57, Bias: -0.001763, T: 5043380, Avg. loss: 0.863289\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.44, NNZs: 57, Bias: 0.045884, T: 5295549, Avg. loss: 0.862420\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.35, NNZs: 57, Bias: 0.060334, T: 5547718, Avg. loss: 0.860599\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.34, NNZs: 57, Bias: 0.006472, T: 5799887, Avg. loss: 0.860554\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.35, NNZs: 57, Bias: 0.062642, T: 6052056, Avg. loss: 0.858793\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.27, NNZs: 57, Bias: 0.032010, T: 6304225, Avg. loss: 0.857889\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.29, NNZs: 57, Bias: 0.095226, T: 6556394, Avg. loss: 0.857841\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.28, NNZs: 57, Bias: 0.087734, T: 6808563, Avg. loss: 0.856416\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.26, NNZs: 57, Bias: 0.107777, T: 7060732, Avg. loss: 0.856175\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.21, NNZs: 57, Bias: 0.094958, T: 7312901, Avg. loss: 0.856241\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.21, NNZs: 57, Bias: -0.062388, T: 7565070, Avg. loss: 0.854789\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.23, NNZs: 57, Bias: 0.000317, T: 7817239, Avg. loss: 0.854930\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.21, NNZs: 57, Bias: 0.028092, T: 8069408, Avg. loss: 0.854104\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.20, NNZs: 57, Bias: 0.038548, T: 8321577, Avg. loss: 0.853416\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.20, NNZs: 57, Bias: 0.047190, T: 8573746, Avg. loss: 0.852832\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.18, NNZs: 57, Bias: 0.133613, T: 8825915, Avg. loss: 0.853116\n",
      "Total training time: 3.61 seconds.\n",
      "Convergence after 35 epochs took 3.61 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(verbose=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_svm = SGDClassifier(verbose = 10)\n",
    "classifier_svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f8392e7eeb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(classifier_svm, X_test_scaled, y_test, values_format ='d', display_labels = ['Not Approved','Approved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SGDClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.52      0.65     69617\n",
      "           1       0.21      0.62      0.32     14440\n",
      "\n",
      "    accuracy                           0.54     84057\n",
      "   macro avg       0.54      0.57      0.49     84057\n",
      "weighted avg       0.76      0.54      0.60     84057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "y_pred = classifier_svm.predict(X_test)\n",
    "print(classification_report(y_pred,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearchCV for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1152 candidates, totalling 5760 fits\n",
      "[CV 1/5; 1/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 2/5; 1/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 3/5; 1/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 1/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 1/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001;, score=0.615 total time=  22.4s\n",
      "[CV 5/5; 1/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 2/5; 1/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001;, score=0.614 total time=  27.2s\n",
      "[CV 1/5; 2/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 1/5; 2/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001;, score=0.609 total time=   8.2s\n",
      "[CV 2/5; 2/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 1/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001;, score=0.612 total time=  15.4s\n",
      "[CV 3/5; 2/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 1/5; 1/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001;, score=0.615 total time=  47.8s\n",
      "[CV 4/5; 2/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 2/5; 2/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001;, score=0.614 total time=  12.6s\n",
      "[CV 5/5; 2/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 2/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001;, score=0.611 total time=   9.7s\n",
      "[CV 1/5; 3/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 1/5; 3/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01;, score=0.610 total time=   7.9s\n",
      "[CV 2/5; 3/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 2/5; 3/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01;, score=0.610 total time=   7.5s\n",
      "[CV 3/5; 3/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 3/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01;, score=0.608 total time=   7.3s\n",
      "[CV 4/5; 3/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 2/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001;, score=0.614 total time=  51.5s\n",
      "[CV 5/5; 3/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 4/5; 3/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01;, score=0.614 total time=  10.7s\n",
      "[CV 1/5; 4/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 1/5; 4/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1;, score=0.609 total time=   5.2s\n",
      "[CV 2/5; 4/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 5/5; 3/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.01;, score=0.603 total time=   8.5s\n",
      "[CV 3/5; 4/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 2/5; 4/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1;, score=0.594 total time=   5.2s\n",
      "[CV 4/5; 4/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 3/5; 4/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1;, score=0.611 total time=   6.3s\n",
      "[CV 5/5; 4/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 4/5; 4/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1;, score=0.605 total time=   2.8s\n",
      "[CV 1/5; 5/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 5/5; 4/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.1;, score=0.603 total time=   3.1s\n",
      "[CV 2/5; 5/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 1/5; 5/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1;, score=0.606 total time=   3.1s\n",
      "[CV 3/5; 5/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 2/5; 5/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1;, score=0.603 total time=   3.1s\n",
      "[CV 4/5; 5/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 3/5; 5/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1;, score=0.605 total time=   3.3s\n",
      "[CV 5/5; 5/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 4/5; 5/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1;, score=0.600 total time=   3.1s\n",
      "[CV 1/5; 6/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 5/5; 5/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1;, score=0.601 total time=   3.0s\n",
      "[CV 2/5; 6/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 1/5; 6/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10;, score=0.595 total time=   2.5s\n",
      "[CV 3/5; 6/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 4/5; 2/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.001;, score=0.619 total time= 1.2min\n",
      "[CV 4/5; 6/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 2/5; 6/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10;, score=0.605 total time=   2.5s\n",
      "[CV 5/5; 6/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 3/5; 6/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10;, score=0.606 total time=   2.4s\n",
      "[CV 1/5; 7/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 4/5; 6/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10;, score=0.605 total time=   2.5s\n",
      "[CV 2/5; 7/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 5/5; 6/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=10;, score=0.605 total time=   2.5s\n",
      "[CV 3/5; 7/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 1/5; 7/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100;, score=0.611 total time=   2.5s\n",
      "[CV 4/5; 7/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 2/5; 7/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100;, score=0.608 total time=   2.6s\n",
      "[CV 5/5; 7/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 3/5; 7/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100;, score=0.595 total time=   2.5s\n",
      "[CV 1/5; 8/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 4/5; 7/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100;, score=0.607 total time=   2.6s\n",
      "[CV 2/5; 8/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 5/5; 7/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=100;, score=0.595 total time=   2.6s\n",
      "[CV 3/5; 8/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 1/5; 8/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000;, score=0.606 total time=   2.7s\n",
      "[CV 4/5; 8/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 2/5; 8/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000;, score=0.594 total time=   2.9s\n",
      "[CV 5/5; 8/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 3/5; 8/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000;, score=0.605 total time=   2.9s\n",
      "[CV 1/5; 9/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 4/5; 8/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000;, score=0.609 total time=   3.0s\n",
      "[CV 2/5; 9/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 5/5; 8/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=1000;, score=0.601 total time=   3.0s\n",
      "[CV 3/5; 9/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 1/5; 9/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001;, score=0.614 total time=  12.9s\n",
      "[CV 4/5; 9/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 2/5; 9/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001;, score=0.614 total time=  14.5s\n",
      "[CV 5/5; 9/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 3/5; 9/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001;, score=0.614 total time=  15.8s\n",
      "[CV 1/5; 10/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 1/5; 10/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001;, score=0.611 total time=   5.6s\n",
      "[CV 2/5; 10/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 4/5; 9/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001;, score=0.617 total time=  18.0s\n",
      "[CV 3/5; 10/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 2/5; 10/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001;, score=0.606 total time=   7.0s\n",
      "[CV 4/5; 10/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 5/5; 9/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.0001;, score=0.612 total time=  16.6s\n",
      "[CV 5/5; 10/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 4/5; 10/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001;, score=0.613 total time=   5.6s\n",
      "[CV 1/5; 11/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01\n",
      "[CV 5/5; 10/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001;, score=0.609 total time=   5.9s\n",
      "[CV 2/5; 11/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01\n",
      "[CV 3/5; 10/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.001;, score=0.608 total time=   6.3s\n",
      "[CV 3/5; 11/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01\n",
      "[CV 1/5; 11/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01;, score=0.600 total time=   3.1s\n",
      "[CV 4/5; 11/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01\n",
      "[CV 2/5; 11/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01;, score=0.603 total time=   3.2s\n",
      "[CV 5/5; 11/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01\n",
      "[CV 3/5; 11/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01;, score=0.602 total time=   3.2s\n",
      "[CV 1/5; 12/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 1/5; 12/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1;, score=0.576 total time=   1.9s\n",
      "[CV 2/5; 12/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 4/5; 11/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01;, score=0.605 total time=   2.8s\n",
      "[CV 3/5; 12/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 5/5; 11/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.01;, score=0.592 total time=   2.5s\n",
      "[CV 4/5; 12/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 2/5; 12/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1;, score=0.599 total time=   2.2s\n",
      "[CV 5/5; 12/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 3/5; 12/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1;, score=0.598 total time=   2.1s\n",
      "[CV 1/5; 13/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1\n",
      "[CV 4/5; 12/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1;, score=0.594 total time=   2.2s\n",
      "[CV 2/5; 13/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1\n",
      "[CV 5/5; 12/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=0.1;, score=0.595 total time=   2.0s\n",
      "[CV 3/5; 13/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1\n",
      "[CV 1/5; 13/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1;, score=0.602 total time=   2.1s\n",
      "[CV 4/5; 13/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1\n",
      "[CV 2/5; 13/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1;, score=0.582 total time=   2.1s\n",
      "[CV 5/5; 13/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1\n",
      "[CV 3/5; 13/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1;, score=0.599 total time=   2.0s\n",
      "[CV 1/5; 14/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 4/5; 13/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1;, score=0.599 total time=   1.9s\n",
      "[CV 2/5; 14/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 5/5; 13/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1;, score=0.601 total time=   2.0s\n",
      "[CV 3/5; 14/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 1/5; 14/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10;, score=0.592 total time=   2.0s\n",
      "[CV 4/5; 14/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 2/5; 14/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10;, score=0.581 total time=   1.9s\n",
      "[CV 5/5; 14/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 3/5; 14/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10;, score=0.594 total time=   2.1s\n",
      "[CV 1/5; 15/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 4/5; 14/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10;, score=0.593 total time=   2.1s\n",
      "[CV 2/5; 15/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 5/5; 14/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=10;, score=0.589 total time=   2.0s\n",
      "[CV 3/5; 15/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 1/5; 15/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100;, score=0.597 total time=   2.0s\n",
      "[CV 4/5; 15/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 2/5; 15/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100;, score=0.580 total time=   2.8s\n",
      "[CV 5/5; 15/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 3/5; 15/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100;, score=0.597 total time=   3.0s\n",
      "[CV 1/5; 16/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 4/5; 15/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100;, score=0.595 total time=   3.0s\n",
      "[CV 2/5; 16/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 5/5; 15/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=100;, score=0.600 total time=   3.0s\n",
      "[CV 3/5; 16/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 1/5; 16/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000;, score=0.592 total time=   3.3s\n",
      "[CV 4/5; 16/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 2/5; 16/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000;, score=0.601 total time=   3.3s\n",
      "[CV 5/5; 16/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 3/5; 16/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000;, score=0.571 total time=   2.4s\n",
      "[CV 1/5; 17/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 4/5; 16/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000;, score=0.592 total time=   2.1s\n",
      "[CV 2/5; 17/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 5/5; 16/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2, tol=1000;, score=0.584 total time=   2.0s\n",
      "[CV 3/5; 17/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 2/5; 17/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.612 total time=  17.7s\n",
      "[CV 4/5; 17/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 1/5; 17/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.614 total time=  19.2s\n",
      "[CV 5/5; 17/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 3/5; 17/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.614 total time=  24.1s\n",
      "[CV 1/5; 18/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 1/5; 18/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.613 total time=   9.2s\n",
      "[CV 2/5; 18/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 5/5; 17/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.613 total time=  16.2s\n",
      "[CV 3/5; 18/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 4/5; 17/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.615 total time=  17.9s\n",
      "[CV 4/5; 18/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 2/5; 18/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.612 total time=   6.6s\n",
      "[CV 5/5; 18/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 3/5; 18/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.609 total time=   6.4s\n",
      "[CV 1/5; 19/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 4/5; 18/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.610 total time=   6.4s\n",
      "[CV 2/5; 19/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 1/5; 19/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.607 total time=   3.5s\n",
      "[CV 3/5; 19/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 2/5; 19/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.606 total time=   3.5s\n",
      "[CV 4/5; 19/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 5/5; 18/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.610 total time=   6.2s\n",
      "[CV 5/5; 19/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 3/5; 19/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.605 total time=   3.5s\n",
      "[CV 1/5; 20/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 4/5; 19/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.607 total time=   3.6s\n",
      "[CV 2/5; 20/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 5/5; 19/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.609 total time=   3.5s\n",
      "[CV 3/5; 20/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 1/5; 20/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.605 total time=   2.8s\n",
      "[CV 4/5; 20/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 2/5; 20/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.604 total time=   2.8s\n",
      "[CV 5/5; 20/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 3/5; 20/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.606 total time=   3.1s\n",
      "[CV 1/5; 21/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 4/5; 20/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.608 total time=   3.4s\n",
      "[CV 2/5; 21/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 5/5; 20/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.603 total time=   3.5s\n",
      "[CV 3/5; 21/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 1/5; 21/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1;, score=0.598 total time=   3.2s\n",
      "[CV 4/5; 21/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 2/5; 21/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1;, score=0.608 total time=   3.0s\n",
      "[CV 5/5; 21/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 3/5; 21/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1;, score=0.614 total time=   2.9s\n",
      "[CV 1/5; 22/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 4/5; 21/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1;, score=0.603 total time=   2.9s\n",
      "[CV 2/5; 22/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 5/5; 21/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1;, score=0.607 total time=   2.9s\n",
      "[CV 3/5; 22/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 1/5; 22/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10;, score=0.599 total time=   2.5s\n",
      "[CV 4/5; 22/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 2/5; 22/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10;, score=0.606 total time=   2.5s\n",
      "[CV 5/5; 22/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 3/5; 22/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10;, score=0.585 total time=   2.5s\n",
      "[CV 1/5; 23/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 4/5; 22/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10;, score=0.611 total time=   2.5s\n",
      "[CV 2/5; 23/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 5/5; 22/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=10;, score=0.603 total time=   2.5s\n",
      "[CV 3/5; 23/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 1/5; 23/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100;, score=0.613 total time=   2.5s\n",
      "[CV 4/5; 23/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 2/5; 23/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100;, score=0.600 total time=   2.5s\n",
      "[CV 5/5; 23/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 3/5; 23/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100;, score=0.603 total time=   2.5s\n",
      "[CV 1/5; 24/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 4/5; 23/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100;, score=0.612 total time=   3.2s\n",
      "[CV 2/5; 24/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 5/5; 23/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=100;, score=0.606 total time=   3.2s\n",
      "[CV 3/5; 24/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 1/5; 24/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.608 total time=   3.5s\n",
      "[CV 4/5; 24/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 2/5; 24/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.599 total time=   3.1s\n",
      "[CV 5/5; 24/1152] START alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 3/5; 24/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.606 total time=   3.1s\n",
      "[CV 1/5; 25/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 24/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.604 total time=   2.9s\n",
      "[CV 2/5; 25/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 5/5; 24/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.599 total time=   2.7s\n",
      "[CV 3/5; 25/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 1/5; 25/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001;, score=0.614 total time=  16.5s\n",
      "[CV 4/5; 25/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 25/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001;, score=0.617 total time=  16.3s\n",
      "[CV 5/5; 25/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 2/5; 25/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001;, score=0.615 total time=  39.6s\n",
      "[CV 1/5; 26/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 25/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001;, score=0.613 total time=  18.2s\n",
      "[CV 2/5; 26/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 2/5; 26/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001;, score=0.613 total time=  14.6s\n",
      "[CV 3/5; 26/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 3/5; 1/1152] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1, tol=0.0001;, score=0.615 total time= 5.8min\n",
      "[CV 4/5; 26/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 3/5; 26/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001;, score=0.613 total time=  25.0s\n",
      "[CV 5/5; 26/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 26/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001;, score=0.612 total time=   6.8s\n",
      "[CV 1/5; 27/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 1/5; 27/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01;, score=0.600 total time=   3.4s\n",
      "[CV 2/5; 27/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 2/5; 27/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01;, score=0.600 total time=   3.4s\n",
      "[CV 3/5; 27/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 27/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01;, score=0.606 total time=   3.9s\n",
      "[CV 4/5; 27/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 4/5; 27/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01;, score=0.613 total time=   7.3s\n",
      "[CV 5/5; 27/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 5/5; 27/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.01;, score=0.608 total time=   3.5s\n",
      "[CV 1/5; 28/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1\n",
      "[CV 1/5; 28/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1;, score=0.602 total time=   3.4s\n",
      "[CV 2/5; 28/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1\n",
      "[CV 2/5; 28/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1;, score=0.606 total time=   3.1s\n",
      "[CV 3/5; 28/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1\n",
      "[CV 3/5; 28/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1;, score=0.607 total time=   4.0s\n",
      "[CV 4/5; 28/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1\n",
      "[CV 4/5; 26/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001;, score=0.618 total time=  49.1s\n",
      "[CV 5/5; 28/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1\n",
      "[CV 4/5; 28/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1;, score=0.612 total time=   3.0s\n",
      "[CV 1/5; 29/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1\n",
      "[CV 5/5; 28/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.1;, score=0.608 total time=   3.1s\n",
      "[CV 2/5; 29/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1\n",
      "[CV 1/5; 29/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1;, score=0.611 total time=   2.8s\n",
      "[CV 3/5; 29/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1\n",
      "[CV 2/5; 29/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1;, score=0.611 total time=   2.8s\n",
      "[CV 4/5; 29/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1\n",
      "[CV 3/5; 29/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1;, score=0.609 total time=   2.8s\n",
      "[CV 5/5; 29/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1\n",
      "[CV 4/5; 29/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1;, score=0.598 total time=   2.8s\n",
      "[CV 1/5; 30/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10\n",
      "[CV 5/5; 29/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1;, score=0.608 total time=   2.8s\n",
      "[CV 2/5; 30/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10\n",
      "[CV 1/5; 30/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10;, score=0.608 total time=   2.5s\n",
      "[CV 3/5; 30/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10\n",
      "[CV 2/5; 30/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10;, score=0.593 total time=   3.1s\n",
      "[CV 4/5; 30/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10\n",
      "[CV 3/5; 30/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10;, score=0.608 total time=   3.3s\n",
      "[CV 5/5; 30/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10\n",
      "[CV 4/5; 30/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10;, score=0.607 total time=   3.0s\n",
      "[CV 1/5; 31/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100\n",
      "[CV 5/5; 30/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=10;, score=0.606 total time=   2.8s\n",
      "[CV 2/5; 31/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100\n",
      "[CV 1/5; 31/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100;, score=0.610 total time=   2.7s\n",
      "[CV 3/5; 31/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100\n",
      "[CV 2/5; 31/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100;, score=0.594 total time=   2.8s\n",
      "[CV 4/5; 31/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100\n",
      "[CV 3/5; 31/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100;, score=0.598 total time=   2.8s\n",
      "[CV 5/5; 31/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100\n",
      "[CV 4/5; 31/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100;, score=0.610 total time=   2.7s\n",
      "[CV 1/5; 32/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000\n",
      "[CV 5/5; 31/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=100;, score=0.605 total time=   2.4s\n",
      "[CV 2/5; 32/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000\n",
      "[CV 1/5; 32/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000;, score=0.611 total time=   2.4s\n",
      "[CV 3/5; 32/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000\n",
      "[CV 2/5; 32/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000;, score=0.601 total time=   2.5s\n",
      "[CV 4/5; 32/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000\n",
      "[CV 3/5; 32/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000;, score=0.603 total time=   2.5s\n",
      "[CV 5/5; 32/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000\n",
      "[CV 4/5; 32/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000;, score=0.597 total time=   2.5s\n",
      "[CV 1/5; 33/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001\n",
      "[CV 5/5; 32/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=1000;, score=0.604 total time=   2.5s\n",
      "[CV 2/5; 33/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001\n",
      "[CV 2/5; 33/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001;, score=0.612 total time=  10.1s\n",
      "[CV 3/5; 33/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001\n",
      "[CV 1/5; 33/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001;, score=0.614 total time=  12.8s\n",
      "[CV 4/5; 33/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001\n",
      "[CV 3/5; 33/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001;, score=0.611 total time=  10.6s\n",
      "[CV 5/5; 33/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001\n",
      "[CV 4/5; 33/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001;, score=0.615 total time=  13.7s\n",
      "[CV 1/5; 34/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001\n",
      "[CV 1/5; 34/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001;, score=0.607 total time=   5.2s\n",
      "[CV 2/5; 34/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001\n",
      "[CV 5/5; 33/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.0001;, score=0.612 total time=  14.4s\n",
      "[CV 3/5; 34/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001\n",
      "[CV 2/5; 34/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001;, score=0.611 total time=   4.9s\n",
      "[CV 4/5; 34/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001\n",
      "[CV 3/5; 34/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001;, score=0.609 total time=   5.6s\n",
      "[CV 5/5; 34/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001\n",
      "[CV 4/5; 34/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001;, score=0.612 total time=   5.2s\n",
      "[CV 1/5; 35/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01\n",
      "[CV 1/5; 35/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01;, score=0.604 total time=   2.7s\n",
      "[CV 2/5; 35/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01\n",
      "[CV 5/5; 34/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.001;, score=0.609 total time=   5.8s\n",
      "[CV 3/5; 35/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01\n",
      "[CV 2/5; 35/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01;, score=0.601 total time=   2.8s\n",
      "[CV 4/5; 35/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01\n",
      "[CV 3/5; 35/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01;, score=0.597 total time=   2.7s\n",
      "[CV 5/5; 35/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01\n",
      "[CV 4/5; 35/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01;, score=0.605 total time=   3.0s\n",
      "[CV 1/5; 36/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1\n",
      "[CV 5/5; 35/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.01;, score=0.599 total time=   2.5s\n",
      "[CV 1/5; 36/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1;, score=0.602 total time=   1.8s\n",
      "[CV 2/5; 36/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1\n",
      "[CV 3/5; 36/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1\n",
      "[CV 2/5; 36/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1;, score=0.595 total time=   1.7s\n",
      "[CV 3/5; 36/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1;, score=0.593 total time=   1.7s\n",
      "[CV 4/5; 36/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1\n",
      "[CV 5/5; 36/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1\n",
      "[CV 4/5; 36/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1;, score=0.601 total time=   1.9s\n",
      "[CV 1/5; 37/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1\n",
      "[CV 5/5; 36/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=0.1;, score=0.602 total time=   1.9s\n",
      "[CV 2/5; 37/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1\n",
      "[CV 1/5; 37/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1;, score=0.598 total time=   2.1s\n",
      "[CV 3/5; 37/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1\n",
      "[CV 2/5; 37/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1;, score=0.596 total time=   2.2s\n",
      "[CV 4/5; 37/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1\n",
      "[CV 3/5; 37/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1;, score=0.592 total time=   1.9s\n",
      "[CV 5/5; 37/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1\n",
      "[CV 4/5; 37/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1;, score=0.595 total time=   1.9s\n",
      "[CV 1/5; 38/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10\n",
      "[CV 1/5; 38/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10;, score=0.590 total time=   1.6s\n",
      "[CV 2/5; 38/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10\n",
      "[CV 5/5; 37/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1;, score=0.594 total time=   1.7s\n",
      "[CV 3/5; 38/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10\n",
      "[CV 2/5; 38/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10;, score=0.603 total time=   1.6s\n",
      "[CV 3/5; 38/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10;, score=0.592 total time=   1.6s\n",
      "[CV 4/5; 38/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10\n",
      "[CV 5/5; 38/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10\n",
      "[CV 4/5; 38/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10;, score=0.598 total time=   1.6s\n",
      "[CV 5/5; 38/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=10;, score=0.584 total time=   1.6s\n",
      "[CV 1/5; 39/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100\n",
      "[CV 2/5; 39/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100\n",
      "[CV 1/5; 39/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100;, score=0.597 total time=   1.5s\n",
      "[CV 2/5; 39/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100;, score=0.588 total time=   1.6s\n",
      "[CV 3/5; 39/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100\n",
      "[CV 4/5; 39/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100\n",
      "[CV 3/5; 39/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100;, score=0.592 total time=   1.5s\n",
      "[CV 4/5; 39/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100;, score=0.598 total time=   1.6s\n",
      "[CV 5/5; 39/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100\n",
      "[CV 1/5; 40/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000\n",
      "[CV 5/5; 39/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=100;, score=0.598 total time=   1.6s\n",
      "[CV 1/5; 40/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000;, score=0.587 total time=   1.6s\n",
      "[CV 2/5; 40/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000\n",
      "[CV 3/5; 40/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000\n",
      "[CV 2/5; 40/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000;, score=0.592 total time=   1.6s\n",
      "[CV 3/5; 40/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000;, score=0.597 total time=   1.5s\n",
      "[CV 5/5; 40/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000\n",
      "[CV 4/5; 40/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000\n",
      "[CV 4/5; 40/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000;, score=0.592 total time=   1.6s\n",
      "[CV 5/5; 40/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2, tol=1000;, score=0.586 total time=   1.6s\n",
      "[CV 2/5; 41/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001\n",
      "[CV 1/5; 41/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001\n",
      "[CV 2/5; 41/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001;, score=0.616 total time=  18.8s\n",
      "[CV 3/5; 41/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001\n",
      "[CV 1/5; 41/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001;, score=0.616 total time=  20.3s\n",
      "[CV 4/5; 41/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001\n",
      "[CV 4/5; 41/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001;, score=0.617 total time=  18.7s\n",
      "[CV 5/5; 41/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001\n",
      "[CV 3/5; 41/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001;, score=0.612 total time=  20.9s\n",
      "[CV 1/5; 42/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001\n",
      "[CV 1/5; 42/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001;, score=0.613 total time=   7.4s\n",
      "[CV 2/5; 42/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001\n",
      "[CV 2/5; 42/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001;, score=0.611 total time=   7.5s\n",
      "[CV 3/5; 42/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001\n",
      "[CV 5/5; 41/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.0001;, score=0.613 total time=  18.4s\n",
      "[CV 4/5; 42/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001\n",
      "[CV 3/5; 42/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001;, score=0.608 total time=   6.9s\n",
      "[CV 5/5; 42/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001\n",
      "[CV 4/5; 42/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001;, score=0.608 total time=   6.5s\n",
      "[CV 1/5; 43/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01\n",
      "[CV 1/5; 43/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01;, score=0.604 total time=   3.6s\n",
      "[CV 2/5; 43/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01\n",
      "[CV 5/5; 42/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.001;, score=0.613 total time=   7.5s\n",
      "[CV 3/5; 43/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01\n",
      "[CV 2/5; 43/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01;, score=0.605 total time=   3.4s\n",
      "[CV 4/5; 43/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01\n",
      "[CV 3/5; 43/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01;, score=0.603 total time=   3.3s\n",
      "[CV 5/5; 43/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01\n",
      "[CV 4/5; 43/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01;, score=0.609 total time=   3.5s\n",
      "[CV 1/5; 44/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1\n",
      "[CV 5/5; 43/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.01;, score=0.610 total time=   3.6s\n",
      "[CV 2/5; 44/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1\n",
      "[CV 1/5; 44/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1;, score=0.596 total time=   2.8s\n",
      "[CV 3/5; 44/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1\n",
      "[CV 2/5; 44/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1;, score=0.601 total time=   2.8s\n",
      "[CV 4/5; 44/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1\n",
      "[CV 3/5; 44/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1;, score=0.605 total time=   2.8s\n",
      "[CV 5/5; 44/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1\n",
      "[CV 4/5; 44/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1;, score=0.608 total time=   3.1s\n",
      "[CV 1/5; 45/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1\n",
      "[CV 5/5; 44/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=0.1;, score=0.607 total time=   3.4s\n",
      "[CV 2/5; 45/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1\n",
      "[CV 1/5; 45/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1;, score=0.608 total time=   3.0s\n",
      "[CV 3/5; 45/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1\n",
      "[CV 2/5; 45/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1;, score=0.610 total time=   2.9s\n",
      "[CV 4/5; 45/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1\n",
      "[CV 3/5; 45/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1;, score=0.606 total time=   2.9s\n",
      "[CV 5/5; 45/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1\n",
      "[CV 4/5; 45/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1;, score=0.599 total time=   3.2s\n",
      "[CV 1/5; 46/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10\n",
      "[CV 5/5; 45/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1;, score=0.605 total time=   3.2s\n",
      "[CV 2/5; 46/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10\n",
      "[CV 1/5; 46/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10;, score=0.606 total time=   2.5s\n",
      "[CV 3/5; 46/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10\n",
      "[CV 2/5; 46/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10;, score=0.604 total time=   2.6s\n",
      "[CV 4/5; 46/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10\n",
      "[CV 1/5; 26/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.001;, score=0.618 total time= 4.8min\n",
      "[CV 5/5; 46/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10\n",
      "[CV 3/5; 46/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10;, score=0.596 total time=   2.6s\n",
      "[CV 1/5; 47/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100\n",
      "[CV 4/5; 46/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10;, score=0.610 total time=   2.8s\n",
      "[CV 2/5; 47/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100\n",
      "[CV 5/5; 46/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=10;, score=0.592 total time=   2.9s\n",
      "[CV 3/5; 47/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100\n",
      "[CV 1/5; 47/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100;, score=0.601 total time=   2.9s\n",
      "[CV 4/5; 47/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100\n",
      "[CV 2/5; 47/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100;, score=0.608 total time=   2.7s\n",
      "[CV 5/5; 47/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100\n",
      "[CV 3/5; 47/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100;, score=0.603 total time=   2.6s\n",
      "[CV 1/5; 48/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000\n",
      "[CV 4/5; 47/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100;, score=0.600 total time=   2.6s\n",
      "[CV 2/5; 48/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000\n",
      "[CV 5/5; 47/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=100;, score=0.607 total time=   2.5s\n",
      "[CV 3/5; 48/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000\n",
      "[CV 1/5; 48/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000;, score=0.594 total time=   2.6s\n",
      "[CV 4/5; 48/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000\n",
      "[CV 2/5; 48/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000;, score=0.598 total time=   2.6s\n",
      "[CV 5/5; 48/1152] START alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000\n",
      "[CV 3/5; 48/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000;, score=0.598 total time=   2.6s\n",
      "[CV 1/5; 49/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 48/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000;, score=0.611 total time=   2.7s\n",
      "[CV 2/5; 49/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001\n",
      "[CV 5/5; 48/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet, tol=1000;, score=0.593 total time=   2.6s\n",
      "[CV 3/5; 49/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001\n",
      "[CV 3/5; 49/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001;, score=0.613 total time=  14.6s\n",
      "[CV 4/5; 49/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001\n",
      "[CV 2/5; 49/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001;, score=0.612 total time=  18.4s\n",
      "[CV 5/5; 49/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 49/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001;, score=0.618 total time=  17.1s\n",
      "[CV 1/5; 50/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 49/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001;, score=0.612 total time=  14.0s\n",
      "[CV 2/5; 50/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001\n",
      "[CV 2/5; 50/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001;, score=0.614 total time=   7.0s\n",
      "[CV 3/5; 50/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001\n",
      "[CV 1/5; 50/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001;, score=0.612 total time=  21.2s\n",
      "[CV 4/5; 50/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001\n",
      "[CV 4/5; 50/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001;, score=0.614 total time=   5.8s\n",
      "[CV 5/5; 50/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 50/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001;, score=0.609 total time=   5.5s\n",
      "[CV 1/5; 51/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01\n",
      "[CV 1/5; 51/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01;, score=0.610 total time=   4.0s\n",
      "[CV 2/5; 51/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 50/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.001;, score=0.617 total time=  33.5s\n",
      "[CV 3/5; 51/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01\n",
      "[CV 2/5; 51/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01;, score=0.608 total time=   5.2s\n",
      "[CV 4/5; 51/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 51/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01;, score=0.606 total time=   3.0s\n",
      "[CV 5/5; 51/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01\n",
      "[CV 4/5; 51/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01;, score=0.613 total time=   6.6s\n",
      "[CV 1/5; 52/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1\n",
      "[CV 1/5; 52/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1;, score=0.602 total time=   3.4s\n",
      "[CV 2/5; 52/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1\n",
      "[CV 5/5; 51/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.01;, score=0.612 total time=   9.7s\n",
      "[CV 3/5; 52/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1\n",
      "[CV 2/5; 52/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1;, score=0.610 total time=   3.1s\n",
      "[CV 4/5; 52/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1\n",
      "[CV 4/5; 52/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1;, score=0.604 total time=   2.9s\n",
      "[CV 5/5; 52/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1\n",
      "[CV 3/5; 52/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1;, score=0.607 total time=   4.7s\n",
      "[CV 1/5; 53/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1\n",
      "[CV 5/5; 52/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.1;, score=0.608 total time=   2.9s\n",
      "[CV 2/5; 53/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1\n",
      "[CV 1/5; 53/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1;, score=0.591 total time=   2.9s\n",
      "[CV 3/5; 53/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1\n",
      "[CV 2/5; 53/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1;, score=0.609 total time=   2.8s\n",
      "[CV 4/5; 53/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1\n",
      "[CV 3/5; 53/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1;, score=0.599 total time=   2.8s\n",
      "[CV 5/5; 53/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1\n",
      "[CV 4/5; 53/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1;, score=0.611 total time=   3.0s\n",
      "[CV 1/5; 54/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10\n",
      "[CV 5/5; 53/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1;, score=0.605 total time=   3.0s\n",
      "[CV 2/5; 54/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10\n",
      "[CV 1/5; 54/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10;, score=0.608 total time=   2.9s\n",
      "[CV 3/5; 54/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10\n",
      "[CV 2/5; 54/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10;, score=0.603 total time=   2.8s\n",
      "[CV 4/5; 54/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10\n",
      "[CV 3/5; 54/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10;, score=0.599 total time=   2.8s\n",
      "[CV 5/5; 54/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10\n",
      "[CV 4/5; 54/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10;, score=0.597 total time=   2.9s\n",
      "[CV 1/5; 55/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100\n",
      "[CV 5/5; 54/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=10;, score=0.600 total time=   2.5s\n",
      "[CV 2/5; 55/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100\n",
      "[CV 1/5; 55/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100;, score=0.603 total time=   2.5s\n",
      "[CV 3/5; 55/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100\n",
      "[CV 2/5; 55/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100;, score=0.595 total time=   2.6s\n",
      "[CV 4/5; 55/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100\n",
      "[CV 3/5; 55/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100;, score=0.611 total time=   2.5s\n",
      "[CV 5/5; 55/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100\n",
      "[CV 4/5; 55/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100;, score=0.606 total time=   2.4s\n",
      "[CV 1/5; 56/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000\n",
      "[CV 5/5; 55/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=100;, score=0.609 total time=   2.4s\n",
      "[CV 2/5; 56/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000\n",
      "[CV 1/5; 56/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000;, score=0.608 total time=   2.4s\n",
      "[CV 3/5; 56/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000\n",
      "[CV 2/5; 56/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000;, score=0.609 total time=   2.5s\n",
      "[CV 4/5; 56/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000\n",
      "[CV 3/5; 56/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000;, score=0.602 total time=   2.4s\n",
      "[CV 5/5; 56/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000\n",
      "[CV 4/5; 56/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000;, score=0.602 total time=   2.4s\n",
      "[CV 1/5; 57/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001\n",
      "[CV 5/5; 56/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=1000;, score=0.596 total time=   2.5s\n",
      "[CV 2/5; 57/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001\n",
      "[CV 1/5; 57/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001;, score=0.611 total time=  11.6s\n",
      "[CV 3/5; 57/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001\n",
      "[CV 2/5; 57/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001;, score=0.613 total time=   9.6s\n",
      "[CV 4/5; 57/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001\n",
      "[CV 3/5; 57/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001;, score=0.610 total time=  10.7s\n",
      "[CV 5/5; 57/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001\n",
      "[CV 4/5; 57/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001;, score=0.618 total time=  12.3s\n",
      "[CV 1/5; 58/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001\n",
      "[CV 1/5; 58/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001;, score=0.605 total time=   5.9s\n",
      "[CV 2/5; 58/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001\n",
      "[CV 5/5; 57/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.0001;, score=0.613 total time=  12.9s\n",
      "[CV 3/5; 58/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001\n",
      "[CV 2/5; 58/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001;, score=0.609 total time=   6.2s\n",
      "[CV 4/5; 58/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001\n",
      "[CV 3/5; 58/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001;, score=0.608 total time=   6.3s\n",
      "[CV 4/5; 58/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001;, score=0.611 total time=   5.4s\n",
      "[CV 5/5; 58/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001\n",
      "[CV 1/5; 59/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01\n",
      "[CV 1/5; 59/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01;, score=0.606 total time=   2.7s\n",
      "[CV 2/5; 59/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01\n",
      "[CV 2/5; 59/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01;, score=0.605 total time=   3.2s\n",
      "[CV 3/5; 59/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01\n",
      "[CV 5/5; 58/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.001;, score=0.608 total time=   6.5s\n",
      "[CV 4/5; 59/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01\n",
      "[CV 3/5; 59/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01;, score=0.598 total time=   2.5s\n",
      "[CV 5/5; 59/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01\n",
      "[CV 4/5; 59/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01;, score=0.606 total time=   2.4s\n",
      "[CV 1/5; 60/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1\n",
      "[CV 1/5; 60/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1;, score=0.592 total time=   1.9s\n",
      "[CV 5/5; 59/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.01;, score=0.607 total time=   2.5s\n",
      "[CV 2/5; 60/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1\n",
      "[CV 3/5; 60/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1\n",
      "[CV 3/5; 60/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1;, score=0.596 total time=   1.8s\n",
      "[CV 4/5; 60/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1\n",
      "[CV 2/5; 60/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1;, score=0.595 total time=   2.0s\n",
      "[CV 5/5; 60/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1\n",
      "[CV 4/5; 60/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1;, score=0.594 total time=   1.8s\n",
      "[CV 1/5; 61/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1\n",
      "[CV 5/5; 60/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=0.1;, score=0.604 total time=   1.9s\n",
      "[CV 2/5; 61/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1\n",
      "[CV 1/5; 61/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1;, score=0.593 total time=   1.8s\n",
      "[CV 3/5; 61/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1\n",
      "[CV 2/5; 61/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1;, score=0.585 total time=   1.8s\n",
      "[CV 4/5; 61/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1\n",
      "[CV 3/5; 61/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1;, score=0.598 total time=   1.8s\n",
      "[CV 5/5; 61/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1\n",
      "[CV 4/5; 61/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1;, score=0.600 total time=   1.8s\n",
      "[CV 1/5; 62/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10\n",
      "[CV 5/5; 61/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1;, score=0.594 total time=   2.1s\n",
      "[CV 2/5; 62/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10\n",
      "[CV 1/5; 62/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10;, score=0.595 total time=   1.9s\n",
      "[CV 3/5; 62/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10\n",
      "[CV 2/5; 62/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10;, score=0.603 total time=   1.8s\n",
      "[CV 4/5; 62/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10\n",
      "[CV 3/5; 62/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10;, score=0.597 total time=   1.8s\n",
      "[CV 5/5; 62/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10\n",
      "[CV 4/5; 62/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10;, score=0.588 total time=   1.6s\n",
      "[CV 5/5; 62/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=10;, score=0.596 total time=   1.6s\n",
      "[CV 1/5; 63/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100\n",
      "[CV 2/5; 63/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100\n",
      "[CV 1/5; 63/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100;, score=0.585 total time=   1.7s\n",
      "[CV 2/5; 63/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100;, score=0.601 total time=   1.7s\n",
      "[CV 3/5; 63/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100\n",
      "[CV 4/5; 63/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100\n",
      "[CV 3/5; 63/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100;, score=0.604 total time=   1.7s\n",
      "[CV 4/5; 63/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100;, score=0.595 total time=   1.7s\n",
      "[CV 5/5; 63/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100\n",
      "[CV 1/5; 64/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000\n",
      "[CV 5/5; 63/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=100;, score=0.596 total time=   1.6s\n",
      "[CV 1/5; 64/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000;, score=0.582 total time=   1.6s\n",
      "[CV 2/5; 64/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000\n",
      "[CV 3/5; 64/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000\n",
      "[CV 2/5; 64/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000;, score=0.595 total time=   1.6s\n",
      "[CV 3/5; 64/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000;, score=0.591 total time=   1.7s\n",
      "[CV 4/5; 64/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000\n",
      "[CV 5/5; 64/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000\n",
      "[CV 4/5; 64/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000;, score=0.600 total time=   1.6s\n",
      "[CV 5/5; 64/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l2, tol=1000;, score=0.580 total time=   1.6s\n",
      "[CV 1/5; 65/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001\n",
      "[CV 2/5; 65/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001\n",
      "[CV 1/5; 65/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001;, score=0.613 total time=  13.7s\n",
      "[CV 3/5; 65/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001\n",
      "[CV 2/5; 65/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001;, score=0.614 total time=  21.6s\n",
      "[CV 4/5; 65/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001\n",
      "[CV 3/5; 65/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001;, score=0.612 total time=  15.3s\n",
      "[CV 5/5; 65/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001\n",
      "[CV 4/5; 65/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001;, score=0.617 total time=  17.3s\n",
      "[CV 1/5; 66/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001\n",
      "[CV 1/5; 66/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001;, score=0.607 total time=   5.1s\n",
      "[CV 2/5; 66/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001\n",
      "[CV 5/5; 65/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.0001;, score=0.614 total time=  19.3s\n",
      "[CV 3/5; 66/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001\n",
      "[CV 2/5; 66/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001;, score=0.612 total time=   5.7s\n",
      "[CV 4/5; 66/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001\n",
      "[CV 3/5; 25/1152] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1, tol=0.0001;, score=0.616 total time= 9.8min\n",
      "[CV 5/5; 66/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001\n",
      "[CV 3/5; 66/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001;, score=0.612 total time=   7.7s\n",
      "[CV 1/5; 67/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01\n",
      "[CV 4/5; 66/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001;, score=0.617 total time=   7.0s\n",
      "[CV 2/5; 67/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01\n",
      "[CV 1/5; 67/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01;, score=0.608 total time=   3.4s\n",
      "[CV 3/5; 67/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01\n",
      "[CV 5/5; 66/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.001;, score=0.606 total time=   6.8s\n",
      "[CV 4/5; 67/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01\n",
      "[CV 2/5; 67/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01;, score=0.606 total time=   3.1s\n",
      "[CV 5/5; 67/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01\n",
      "[CV 3/5; 67/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01;, score=0.606 total time=   3.2s\n",
      "[CV 1/5; 68/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1\n",
      "[CV 5/5; 67/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01;, score=0.609 total time=   3.1s\n",
      "[CV 2/5; 68/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1\n",
      "[CV 4/5; 67/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.01;, score=0.614 total time=   3.5s\n",
      "[CV 3/5; 68/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1\n",
      "[CV 1/5; 68/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1;, score=0.597 total time=   3.0s\n",
      "[CV 4/5; 68/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1\n",
      "[CV 2/5; 68/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1;, score=0.608 total time=   3.0s\n",
      "[CV 5/5; 68/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1\n",
      "[CV 3/5; 68/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1;, score=0.605 total time=   3.0s\n",
      "[CV 1/5; 69/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1\n",
      "[CV 4/5; 68/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1;, score=0.608 total time=   2.7s\n",
      "[CV 2/5; 69/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1\n",
      "[CV 5/5; 68/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=0.1;, score=0.606 total time=   2.8s\n",
      "[CV 3/5; 69/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1\n",
      "[CV 1/5; 69/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1;, score=0.605 total time=   2.8s\n",
      "[CV 4/5; 69/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1\n",
      "[CV 2/5; 69/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1;, score=0.600 total time=   2.8s\n",
      "[CV 5/5; 69/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1\n",
      "[CV 3/5; 69/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1;, score=0.602 total time=   2.8s\n",
      "[CV 1/5; 70/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10\n",
      "[CV 4/5; 69/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1;, score=0.614 total time=   2.9s\n",
      "[CV 2/5; 70/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10\n",
      "[CV 5/5; 69/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1;, score=0.603 total time=   2.9s\n",
      "[CV 3/5; 70/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10\n",
      "[CV 1/5; 70/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10;, score=0.607 total time=   2.5s\n",
      "[CV 4/5; 70/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10\n",
      "[CV 2/5; 70/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10;, score=0.607 total time=   2.5s\n",
      "[CV 5/5; 70/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10\n",
      "[CV 3/5; 70/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10;, score=0.604 total time=   2.9s\n",
      "[CV 1/5; 71/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100\n",
      "[CV 4/5; 70/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10;, score=0.594 total time=   3.0s\n",
      "[CV 2/5; 71/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100\n",
      "[CV 5/5; 70/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=10;, score=0.597 total time=   3.0s\n",
      "[CV 3/5; 71/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100\n",
      "[CV 1/5; 71/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100;, score=0.604 total time=   2.8s\n",
      "[CV 4/5; 71/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100\n",
      "[CV 2/5; 71/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100;, score=0.603 total time=   2.7s\n",
      "[CV 5/5; 71/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100\n",
      "[CV 3/5; 71/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100;, score=0.602 total time=   2.7s\n",
      "[CV 1/5; 72/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000\n",
      "[CV 4/5; 71/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100;, score=0.605 total time=   2.5s\n",
      "[CV 2/5; 72/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000\n",
      "[CV 5/5; 71/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=100;, score=0.587 total time=   2.5s\n",
      "[CV 3/5; 72/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000\n",
      "[CV 1/5; 72/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000;, score=0.608 total time=   2.5s\n",
      "[CV 4/5; 72/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000\n",
      "[CV 2/5; 72/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000;, score=0.608 total time=   2.7s\n",
      "[CV 5/5; 72/1152] START alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000\n",
      "[CV 3/5; 72/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000;, score=0.600 total time=   2.9s\n",
      "[CV 1/5; 73/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 4/5; 72/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000;, score=0.607 total time=   2.9s\n",
      "[CV 2/5; 73/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 5/5; 72/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=elasticnet, tol=1000;, score=0.599 total time=   2.8s\n",
      "[CV 3/5; 73/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 1/5; 73/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001;, score=0.509 total time=  15.7s\n",
      "[CV 4/5; 73/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 3/5; 73/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001;, score=0.525 total time=  58.4s\n",
      "[CV 5/5; 73/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001\n",
      "[CV 5/5; 73/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001;, score=0.456 total time=  14.1s\n",
      "[CV 1/5; 74/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 1/5; 49/1152] END alpha=0.0001, loss=log_loss, max_iter=100000, penalty=l1, tol=0.0001;, score=0.618 total time= 8.5min\n",
      "[CV 2/5; 74/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 73/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001;, score=0.489 total time= 5.2min\n",
      "[CV 3/5; 74/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 73/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.0001;, score=0.494 total time= 5.2min\n",
      "[CV 4/5; 74/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 74/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001;, score=0.480 total time= 5.1min\n",
      "[CV 5/5; 74/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001\n",
      "[CV 3/5; 74/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001;, score=0.528 total time= 1.2min\n",
      "[CV 1/5; 75/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 1/5; 75/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01;, score=0.515 total time=  18.1s\n",
      "[CV 2/5; 75/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 2/5; 74/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001;, score=0.475 total time= 3.7min\n",
      "[CV 3/5; 75/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 3/5; 75/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01;, score=0.500 total time=  19.2s\n",
      "[CV 4/5; 75/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 5/5; 74/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001;, score=0.496 total time= 3.2min\n",
      "[CV 5/5; 75/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01\n",
      "[CV 5/5; 75/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01;, score=0.485 total time=  22.0s\n",
      "[CV 1/5; 76/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 2/5; 75/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01;, score=0.490 total time= 4.4min\n",
      "[CV 2/5; 76/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 74/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.001;, score=0.494 total time= 5.7min\n",
      "[CV 3/5; 76/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1\n",
      "[CV 1/5; 76/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1;, score=0.486 total time= 2.6min\n",
      "[CV 4/5; 76/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 75/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.01;, score=0.498 total time= 6.9min\n",
      "[CV 5/5; 76/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 76/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1;, score=0.509 total time= 7.8min\n",
      "[CV 1/5; 77/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 76/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1;, score=0.543 total time= 7.8min\n",
      "[CV 2/5; 77/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 1/5; 77/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1;, score=0.511 total time=  48.2s\n",
      "[CV 3/5; 77/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 76/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1;, score=0.482 total time= 7.9min\n",
      "[CV 4/5; 77/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 4/5; 77/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1;, score=0.485 total time=  29.6s\n",
      "[CV 5/5; 77/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1\n",
      "[CV 3/5; 77/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1;, score=0.556 total time= 1.5min\n",
      "[CV 1/5; 78/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 5/5; 77/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1;, score=0.492 total time=  25.1s\n",
      "[CV 2/5; 78/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 1/5; 78/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10;, score=0.471 total time=  47.9s\n",
      "[CV 3/5; 78/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 76/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=0.1;, score=0.499 total time= 7.6min\n",
      "[CV 4/5; 78/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 4/5; 78/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10;, score=0.466 total time=   2.7s\n",
      "[CV 5/5; 78/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10\n",
      "[CV 3/5; 78/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10;, score=0.485 total time=  33.1s\n",
      "[CV 1/5; 79/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 1/5; 79/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100;, score=0.506 total time=  26.2s\n",
      "[CV 2/5; 79/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 2/5; 77/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1;, score=0.475 total time= 4.8min\n",
      "[CV 3/5; 79/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 3/5; 79/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100;, score=0.505 total time=  19.0s\n",
      "[CV 4/5; 79/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 2/5; 78/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10;, score=0.520 total time= 4.0min\n",
      "[CV 5/5; 79/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100\n",
      "[CV 5/5; 79/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100;, score=0.503 total time=  22.2s\n",
      "[CV 1/5; 80/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 1/5; 80/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000;, score=0.556 total time=   3.7s\n",
      "[CV 2/5; 80/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 2/5; 79/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100;, score=0.483 total time= 4.1min\n",
      "[CV 3/5; 80/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 3/5; 80/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000;, score=0.517 total time=  15.2s\n",
      "[CV 4/5; 80/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000\n",
      "[CV 4/5; 80/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000;, score=0.513 total time=  34.2s\n",
      "[CV 5/5; 80/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 78/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=10;, score=0.510 total time= 5.6min\n",
      "[CV 1/5; 81/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 79/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=100;, score=0.478 total time= 5.9min\n",
      "[CV 2/5; 81/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 2/5; 80/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000;, score=0.475 total time= 4.7min\n",
      "[CV 3/5; 81/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 81/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001;, score=0.485 total time= 3.8min\n",
      "[CV 4/5; 81/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001\n",
      "[CV 3/5; 81/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001;, score=0.508 total time= 3.1min\n",
      "[CV 5/5; 81/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 81/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001;, score=0.518 total time= 4.0min\n",
      "[CV 1/5; 82/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 80/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l1, tol=1000;, score=0.495 total time= 6.1min\n",
      "[CV 2/5; 82/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 5/5; 81/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001;, score=0.507 total time= 2.2min\n",
      "[CV 3/5; 82/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 81/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.0001;, score=0.518 total time= 4.0min\n",
      "[CV 4/5; 82/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001\n",
      "[CV 1/5; 82/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001;, score=0.500 total time= 3.0min\n",
      "[CV 5/5; 82/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 82/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001;, score=0.479 total time= 3.9min\n",
      "[CV 1/5; 83/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 82/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001;, score=0.508 total time= 4.3min\n",
      "[CV 2/5; 83/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 82/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001;, score=0.494 total time= 4.3min\n",
      "[CV 3/5; 83/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 82/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.001;, score=0.523 total time= 4.6min\n",
      "[CV 4/5; 83/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 83/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01;, score=0.509 total time= 4.4min\n",
      "[CV 5/5; 83/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 83/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01;, score=0.522 total time= 4.8min\n",
      "[CV 1/5; 84/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 83/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01;, score=0.493 total time= 4.9min\n",
      "[CV 2/5; 84/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 83/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01;, score=0.500 total time= 5.3min\n",
      "[CV 3/5; 84/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 83/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.01;, score=0.486 total time= 5.4min\n",
      "[CV 4/5; 84/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1\n",
      "[CV 1/5; 84/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1;, score=0.505 total time= 3.5min\n",
      "[CV 5/5; 84/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 84/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1;, score=0.485 total time= 4.5min\n",
      "[CV 1/5; 85/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 84/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1;, score=0.485 total time= 3.6min\n",
      "[CV 2/5; 85/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 84/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1;, score=0.483 total time= 3.4min\n",
      "[CV 3/5; 85/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 84/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=0.1;, score=0.488 total time= 3.3min\n",
      "[CV 4/5; 85/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 85/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1;, score=0.521 total time= 3.4min\n",
      "[CV 5/5; 85/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 85/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1;, score=0.479 total time= 3.5min\n",
      "[CV 1/5; 86/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 85/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1;, score=0.508 total time= 3.6min\n",
      "[CV 2/5; 86/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 85/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1;, score=0.487 total time= 3.6min\n",
      "[CV 3/5; 86/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 85/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1;, score=0.520 total time= 3.5min\n",
      "[CV 4/5; 86/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10\n",
      "[CV 1/5; 86/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10;, score=0.475 total time= 2.5min\n",
      "[CV 5/5; 86/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 86/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10;, score=0.479 total time= 3.4min\n",
      "[CV 1/5; 87/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 86/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10;, score=0.481 total time= 3.4min\n",
      "[CV 2/5; 87/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 5/5; 86/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10;, score=0.482 total time= 2.3min\n",
      "[CV 3/5; 87/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 86/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=10;, score=0.480 total time= 3.5min\n",
      "[CV 4/5; 87/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100\n",
      "[CV 3/5; 87/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100;, score=0.497 total time= 2.2min\n",
      "[CV 5/5; 87/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 87/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100;, score=0.476 total time= 3.5min\n",
      "[CV 1/5; 88/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 87/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100;, score=0.519 total time= 3.6min\n",
      "[CV 2/5; 88/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000\n",
      "[CV 5/5; 87/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100;, score=0.526 total time= 2.1min\n",
      "[CV 3/5; 88/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 87/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=100;, score=0.482 total time= 3.4min\n",
      "[CV 4/5; 88/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 88/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000;, score=0.518 total time= 3.2min\n",
      "[CV 5/5; 88/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 88/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000;, score=0.478 total time= 3.3min\n",
      "[CV 1/5; 89/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 88/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000;, score=0.523 total time= 3.4min\n",
      "[CV 2/5; 89/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 88/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000;, score=0.493 total time= 3.4min\n",
      "[CV 3/5; 89/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 3/5; 89/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.509 total time=  10.7s\n",
      "[CV 4/5; 89/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 5/5; 88/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=l2, tol=1000;, score=0.537 total time= 2.4min\n",
      "[CV 5/5; 89/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001\n",
      "[CV 5/5; 89/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.498 total time=   9.2s\n",
      "[CV 1/5; 90/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 1/5; 89/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.500 total time= 3.4min\n",
      "[CV 2/5; 90/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 1/5; 90/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.499 total time= 2.9min\n",
      "[CV 3/5; 90/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 89/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.517 total time= 5.1min\n",
      "[CV 4/5; 90/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 89/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.0001;, score=0.493 total time= 5.0min\n",
      "[CV 5/5; 90/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001\n",
      "[CV 5/5; 90/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.506 total time=  31.3s\n",
      "[CV 1/5; 91/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 90/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.491 total time= 5.0min\n",
      "[CV 2/5; 91/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 4/5; 90/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.515 total time= 3.3min\n",
      "[CV 3/5; 91/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 1/5; 91/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.495 total time= 3.2min\n",
      "[CV 4/5; 91/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 90/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.001;, score=0.479 total time= 5.3min\n",
      "[CV 5/5; 91/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01\n",
      "[CV 4/5; 91/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.512 total time=  18.5s\n",
      "[CV 1/5; 92/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 1/5; 92/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.482 total time=  16.2s\n",
      "[CV 2/5; 92/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 3/5; 91/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.497 total time= 3.1min\n",
      "[CV 3/5; 92/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1\n",
      "[CV 5/5; 91/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.498 total time= 3.2min\n",
      "[CV 4/5; 92/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 91/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.01;, score=0.482 total time= 5.4min\n",
      "[CV 5/5; 92/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 92/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.478 total time= 5.2min\n",
      "[CV 1/5; 93/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 5/5; 92/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.488 total time= 1.9min\n",
      "[CV 2/5; 93/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 4/5; 92/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.520 total time= 3.0min\n",
      "[CV 3/5; 93/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 3/5; 93/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1;, score=0.532 total time=  31.2s\n",
      "[CV 4/5; 93/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1\n",
      "[CV 4/5; 93/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1;, score=0.481 total time=  20.7s\n",
      "[CV 5/5; 93/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 92/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=0.1;, score=0.481 total time= 5.0min\n",
      "[CV 1/5; 94/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 1/5; 94/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10;, score=0.431 total time=  37.8s\n",
      "[CV 2/5; 94/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 5/5; 93/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1;, score=0.474 total time= 3.1min\n",
      "[CV 3/5; 94/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 93/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1;, score=0.522 total time= 5.2min\n",
      "[CV 4/5; 94/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 93/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1;, score=0.509 total time= 5.2min\n",
      "[CV 5/5; 94/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10\n",
      "[CV 5/5; 94/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10;, score=0.487 total time=  15.8s\n",
      "[CV 1/5; 95/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 94/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10;, score=0.485 total time= 5.4min\n",
      "[CV 2/5; 95/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 4/5; 94/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10;, score=0.511 total time= 3.9min\n",
      "[CV 3/5; 95/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 1/5; 95/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100;, score=0.489 total time= 3.7min\n",
      "[CV 4/5; 95/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 94/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=10;, score=0.482 total time= 5.3min\n",
      "[CV 5/5; 95/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100\n",
      "[CV 5/5; 95/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100;, score=0.469 total time=   9.8s\n",
      "[CV 1/5; 96/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 1/5; 96/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.476 total time=   9.3s\n",
      "[CV 2/5; 96/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 3/5; 95/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100;, score=0.493 total time= 3.7min\n",
      "[CV 3/5; 96/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 95/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100;, score=0.494 total time= 5.2min\n",
      "[CV 4/5; 96/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 4/5; 96/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.498 total time=  28.0s\n",
      "[CV 5/5; 96/1152] START alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000\n",
      "[CV 5/5; 96/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.510 total time=  11.7s\n",
      "[CV 1/5; 97/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 1/5; 97/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001;, score=0.511 total time=  45.5s\n",
      "[CV 2/5; 97/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 95/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=100;, score=0.478 total time= 5.3min\n",
      "[CV 3/5; 97/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/.local/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 96/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.480 total time= 5.4min\n",
      "[CV 4/5; 97/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 3/5; 96/1152] END alpha=0.0001, loss=squared_error, max_iter=1000, penalty=elasticnet, tol=1000;, score=0.499 total time= 3.4min\n",
      "[CV 5/5; 97/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001\n",
      "[CV 5/5; 97/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001;, score=0.470 total time=  24.6s\n",
      "[CV 1/5; 98/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 1/5; 98/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001;, score=0.478 total time=   2.8s\n",
      "[CV 2/5; 98/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 4/5; 97/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.0001;, score=0.490 total time= 3.8min\n",
      "[CV 3/5; 98/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 3/5; 98/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001;, score=0.478 total time=  17.9s\n",
      "[CV 4/5; 98/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 4/5; 98/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001;, score=0.498 total time=  58.3s\n",
      "[CV 5/5; 98/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001\n",
      "[CV 5/5; 98/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001;, score=0.489 total time=  20.2s\n",
      "[CV 1/5; 99/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.01\n",
      "[CV 2/5; 98/1152] END alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.001;, score=0.528 total time= 4.5min\n",
      "[CV 2/5; 99/1152] START alpha=0.0001, loss=squared_error, max_iter=10000, penalty=l1, tol=0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlog_loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msquared_error\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mpenalty\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39melasticnet\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.0001\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1000\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mmax_iter\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1000\u001b[39m, \u001b[39m10000\u001b[39m, \u001b[39m100000\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mtol\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.0001\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1000\u001b[39m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimal_params \u001b[39m=\u001b[39m GridSearchCV(SGDClassifier(), param_grid, cv \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, n_jobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/amarjotsinghlohia/Documents/Dissertation/Code/SVM_model.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimal_params\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {'loss': ['log_loss', 'squared_error'], \n",
    "              'penalty': ['l1', 'l2', 'elasticnet'], \n",
    "              'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "              'max_iter': [1000, 10000, 100000], \n",
    "              'tol': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "optimal_params = GridSearchCV(SGDClassifier(), param_grid, cv = 5, verbose = 10, n_jobs = -1)\n",
    "\n",
    "optimal_params.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6069333333333333\n"
     ]
    }
   ],
   "source": [
    "print(optimal_params.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fda63915640>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(optimal_params, X_test_scaled, y_test, values_format ='d', display_labels = ['Not Approved','Approved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarjotsinghlohia/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67      2499\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50      2500\n",
      "   macro avg       0.50      0.25      0.33      2500\n",
      "weighted avg       1.00      0.50      0.67      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "y_pred = optimal_params.predict(X_test)\n",
    "report = classification_report(y_pred,y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "425aa5fac1af2c2b2daa42dcae890837cc1a879be8cd68510719d5942fa4f50e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
